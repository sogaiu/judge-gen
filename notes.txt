# USAGE PATTERNS:
#
# * exploratory tests for one's project
# * examples for one's project
# * standalone repository of examples for another project
# * standalone repository of tests for some project -- idea of not
#   including tests in one's project, but rather have them be in a dedicated
#   project for testing

# LIMITS:
#
# * tends to work for direct subdirectories of the project directory.
#   files can be in deeper directories, but for least hassle, point
#   configuration information at direct subdirectories of project
#   directory.  see details below for more in-depth explanation.
#
# * doesn't clean up temporary files and directories automatically, though
#   running a subsequent set of tests will erase old results and generated
#   tests
#
# * updating an existing judge-gen installation has the downside of
#   potentially erasing previous configuration information because this
#   mostly lives in the runner file itself (with the exception of the
#   name of the file which is used as configuration information)
#
# * likely only works with utf-8

# PERIODIC:
#
# * go through code looking for longish functions and consider whether
#   breaking into pieces is a good idea
#
# * go through code looking for XXX and collect here
#
# * audit / review code for:
#   * use of stdout vs stderr (e.g. prin* vs eprin*)
#   * prefer break over assert in many places?
#   * break without argument
#   * once error-handling approach is settled on, try to follow it

# POSSIBILITIES:
#
# * try to figure out a way to have two (or more) instances of the runner
#   able to coexist in one codebase.
#   * one issue is the potential collisions that might arise because of the
#     use of the same .judge directory.
#   * one idea is to not have a .judge directory but rather a .judge_<name>
#     directory where <name> is taken from the source-of-tests directory
#     associated with a runner.
#   * is it possible that two or more <name>s could collide?  it is
#     conceivable that there are runners with the same name living under
#     different descendant directories of `test`. is this something to be
#     concerned about?  possibly it's not our job to prevent this from
#     happening?
#   * possibly there is some way to encode the path from `test` to the runner
#     into the name that is chosen to be appended to `.judge_`.  it seems
#     like some kind of escaping might be necessary to get this to work to
#     avoid using slash or backslash in the name.
#   * try the following idea...suppose there are two judge-gen runner
#     files in `test`:
#     * `test/a/examples.janet`
#     * `test/b/examples.janet`
#     their corresponding `.judge_*` directory names are then:
#     * `.judge_a,examples`
#     * `.judge_b,examples`
#     in the case that a judge-gen runner has a comma in its name, e.g.:
#     * `test/a/examples,1.janet`
#     the corresponding `.judge_*` directory name is:
#     * `.judge_a,examples,,1`
#     in the case that a judge-gen runner has two successive commas in its
#     name, e.g.:
#     * `test/a/examples,,x.janet`
#     the corresponding `.judge_*` directory name is:
#     * `.judge_a,examples,,,,x`
#     so a single comma is replaced by two commas (escaped)
#   * so it seems workable to have multiple judge-gen instances that refer
#     to the same direct subdirectory of the project directory.  is this
#     actually useful?  if each runner can be configured separately, then
#     yes, it can be.
#
# * multiple runner considerations
#   * use separate judge roots -- use .judge_<name> idea described
#     elsewhere.
#   * reminder that things may work better if each src-dir-name
#     refers to a direct subdirectory of project directory
#
# * the structure and content of the .judge directory is not spelled out so
#   well.  consider remedying.  also mention it should be safe to delete.
#   * timing of creation
#   * content
#     * test files
#     * source files
#     * test results (report + stdout and stderr output)
#
# * consider controlling behavior via env vars -- e.g. disable running.
#   perhaps env vars can be used instead of a config file.  then each
#   user is free to make multiple launch scripts that set env vars
#   before executing `jpm test`
#   * lauching via a script has another potential benefit which is
#     that additional command line options can be meaningfully
#     specified as `jpm test` doesn't allow for passing additional
#     command line arguments.
#   * benefit over command line arguments here is two-fold:
#     * works with `jpm test`
#     * don't have to think about names for subcommands, options, etc.
#   * a drawback might be security-related -- if there is no way to
#     indicate ignoring env vars
#
# * would using recent janet-peg for parsing as well as rewriting
#   source be any better than the current approach?
#   * possibly easier to transform and output (assuming something like
#     janet-zip or similar exists?)
#     * semantics of "judge-" file may more closely match intent if
#       comment blocks are unwrapped "inline", though it's also possible
#       that people might not have considered the impacts of evaluating
#       the content of one comment block vs another (e.g. use of the same
#       names)
#     * alternative output formats of tests (e.g. helper.janet, testament,
#       etc.) / conversion might be easier with this approach?
#   * possibly easier to determine line / column info for tests?
#   * examine alc.x-as-tests for hints
#
# * consider what might be done to improve error handling and messages
#   * track and log actual instances
#   * use `file/flush` and friends (e.g. `eflush`) appropriately to get
#     messages to show up in proper order
#   * use convention of capitalizing first word of ordinary stderr output
#     and lowercase first word of `(dyn :debug)` messages?
#     alternatively, prefix debug output with something like "[debug]"
#   * consider whether the following approach is sensible:
#     * use `(error nil)` when producing output is no longer neceesary
#       (i.e. already done), but it's still an error.
#     * use `(error {...})` to pass info back "up" to eventually have
#       the info used in an appropriate message
#     * compare with idea:
#       * in jg/handle_one, instead of `(break false)`, perhaps different
#         values could be returned to indicate different types of errors
#
# * try to "finish" judge-gen and not add anything as much as possible
#   * be able to launch in debug mode (via `:args`) for additional output?
#   * consider an extension mechanism where some extensions are only
#     run via direct execution of the runner:
#     `janet test/runner.janet --extension` (cf. `(dyn :args)`)
#     * may be it's good to have hook "points" in the existing code
#       where extensions can be called from.
#     * some ideas
#       * possible to provide config file / directory via extension?
#       * linting source files via extension?
#       * diagnostic mode might be done to check sanity of configuration,
#         file / directory structure, etc. via extension? collect gotchas,
#         issues, etc. together for consideration of potential things to
#         check for.
#       * trying to run all test files to determine every file that leads
#         to failure (cf. `jpm test` fail early behavior gives an incomplete
#         picture) via extension?
#       * customizing reporting might be implemented via extension?
#       * various "rewriting" targets, e.g. testament, helper.janet
#         for "transition" / exporting purposes
#       * a mode of operation to work on stuff outside a comment block?
#
#         so for example the following at the top-level:
#
#           (def a 1)
#           # => 1
#
#        ought to work as input to jg's transformation process.
#      * a mode of operation to apply runner to a single file
#      * consider different reporting modes for jg.  concrete examples:
#        * for earliest / quickest feedback, have _verify output info as soon
#          as it is available
#        * for slower feedback, have _verify just pass back all test
#          results at the end when they are all ready
#        different receivers might be set up for handling test results, but
#        this may be getting too elaborate at this stage :)
#
# * document the limitation about direct subdirectories of the project
#   directory and the "why".  a direct subdirectory of the project directory
#   has at least the following important properties:
#   * copying the content (contained files and directories) to a sibling
#     directory (one that is also a direct subdirectory of the
#     project directory) doesn't adversely affect most likely (tm)
#     paths in import forms.  at least so far, the probability of
#     the imports still working after copying seems pretty high.
#     one pathological case is an import with the path of the form
#     "../name-a/name-b", where the import is in a file which lives in
#     a directory with name "name-a".  things like this are likely to
#     not work with judge-gen's copying scheme.  it seems unlikely to
#     be a problem though because why wouldn't one express the path
#     like "./name-b" instead?
#   * copying the content (contained files and directories) to a sibling
#     directory also doesn't "leave behind" files and directories that might
#     be necessary for imports to work correctly.  if a non-direct
#     subdirectory of the project directory is specified, there is a chance
#     its content will refer to a sibling's content (which would also be a
#     non-direct subdirectory of the project directory), and judge-gen's
#     copying scheme would fail for this case.
#   if these two points are not issues for one's use case, it may be
#   that one can use a non-direct subdirectory of the project directory
#   (i.e. a "deeper" subdirectory) in the configuration of judge-gen.
#
# * _verify/dump-results uses "%p" in a branch.  determine if this could
#   be a problem but also whether this branch is ever used.
#
# * downside of using `(deep= A B)` with expected value of true is
#   when the test fails, there's not a whole lot of useful information
#   apart from the failure.  is this just a trade-off that has to be
#   made with the current approach?  there is a way to write things
#   that avoids this, but that method requires writing a `def` before
#   the expression to be evaluated and then using the name of the
#   `def` as the expected value.  might be good to add this type of info
#   to documentation.
#
# * consider if there is any benefit in allowing some control over
#   test file execution order, e.g.
#   * sort by name
#   * sort by other criteria
#   * random
#   * custom order
#
# * consider reporting number of tests per file
#
# * configuration ideas
#   * don't make a configuration file for the moment, try to collect
#     usage scenarios, then revisit.
#   * currently, using the name of the runner as a way to indicate which
#     direct subdirectory of the project directory to use as a source of
#     tests.  this can be overriden via configuration in the runner.
#     * still it should be possible to finely configure the runner based
#       on values in the runner (e.g. judge-root)
#     * can put runner in a subdirectory of test if there are name
#       collision issues
#   * motivation for configuration via external file
#     * current method risks losing configuration information upon upgrade
#     * current method leads to likelihood of having to edit
#       test/judge-gen.janet on each upgrade (tedious and error-prone?)
#     * current method risks breaking test/judge-gen.janet because a user
#       typically has to edit it at least once
#   * each runner in test should have a unique name that can be used
#     to look up configuration info in jg.jdn.  if there is no jg.jdn, then
#     the name of the file specifies a subdirectory of the project directory.
#     this last bit helps make it possible for the config file is optional
#     for some cases.
#   * can use one judge-gen.janet per directory of source files, but that
#     would lead to multiple judge-gen.janet files.  perhaps an alternative
#     is to have a single configuration file that could "drive" judge-gen.janet
#     to be used for multple directories.  need to consider multiple
#     judge directories to prevent collisions?
#   * location of config file
#     * project root - e.g. `jg.jdn`
#     * directory with runner - e.g. `test/jg.jdn`, `test/fun/jg.jdn`
#     * could do some kind of search order - e.g. runner dir first, up, etc.

# ISSUES:
#
# * how / whether to handle rather large return values -- load from external
#   file?
#
# * how / whether to try to test output (such as from `print`) -- see
#   spork's `capture-stdout` for an idea
#
# * some existing repositories use an older version of judge-gen, enumerate
#   these and consider migration to newer version.  some things that might
#   need to be done:
#   * count number of tests using old version of judge-gen
#   * ensure tests only use simplified syntax
#   * edit project.janet to remove stale phony targets
#   * add runner to test directory
#   * compare number of tests between old and new versions
#   * possibly remove jg and jg-verdict binaries
#   repositories known to use judge-gen:
#   * clojure-peg
#   * detect-clj-ns
#   * forcett
#   * janet-bits
#   * janet-peg
#   * janet-xmlish
#   * janet-zip
#   * judge-gen
#   * mal-peg
#   * margaret
